AkHQ: http://localhost:8085

SWAGGER: http://localhost:8082/swagger-ui/index.html#/send-controller/send

Em sistemas baseados em Kafka, o consumo de mensagens não implica na remoção dos eventos do tópico. O Kafka funciona como um log distribuído e imutável, no qual as mensagens são armazenadas de forma sequencial e permanecem disponíveis até que sejam removidas por políticas de retenção previamente configuradas, como tempo máximo de armazenamento ou limite de espaço em disco. O papel do consumidor, portanto, não é apagar mensagens, mas sim manter o controle do seu progresso de leitura por meio do offset associado a um consumer group.

Cada consumer group possui seu próprio offset, que indica até qual mensagem o grupo já processou em cada partição do tópico. À medida que a aplicação consome mensagens com sucesso, esse offset é atualizado, avançando no log. O conjunto de mensagens que ainda não foi processado por um consumer group é conhecido como consumer lag, calculado como a diferença entre o último offset disponível no tópico e o offset atual do grupo. É esse lag que normalmente é monitorado em ferramentas como Grafana, onde costuma aparecer como “mensagens disponíveis” ou “eventos pendentes”, diminuindo conforme a aplicação consome os dados.

Essa distinção explica por que, em interfaces administrativas como AKHQ ou Kafdrop, é possível visualizar um número elevado de mensagens armazenadas em um tópico mesmo quando o lag do consumer group é zero. Nessas interfaces, o foco está no histórico do log mantido pelo Kafka, enquanto as métricas exibidas em dashboards operacionais estão orientadas à saúde do consumo e à capacidade da aplicação de acompanhar o volume de eventos produzidos. Assim, a redução do número de mensagens “disponíveis” observada no Grafana não representa a exclusão dos eventos, mas sim o avanço do offset do consumer group.

O crescimento do armazenamento em Kafka é controlado exclusivamente pelas políticas de retenção configuradas em cada tópico. O Kafka remove dados antigos de forma automática e eficiente, apagando segmentos inteiros do log quando os limites de tempo ou tamanho são atingidos, independentemente de as mensagens terem sido consumidas ou não. Esse modelo permite que múltiplos consumer groups processem os mesmos dados de maneira independente, possibilitando reprocessamento, auditoria e diferentes visões de consumo sobre o mesmo fluxo de eventos, sem impacto direto no armazenamento além do que foi definido pela retenção.

Dessa forma, Kafka se posiciona como um mecanismo de transporte e retenção temporária de eventos, e não como um repositório permanente de dados. Em arquiteturas maduras, ele atua como o backbone de eventos, enquanto sistemas de armazenamento de longo prazo, como bancos de dados ou data lakes, são responsáveis pela persistência definitiva das informações. Esse desacoplamento entre produção, consumo e retenção é o que permite ao Kafka escalar de forma previsível e confiável, mesmo em cenários de alto volume de eventos.